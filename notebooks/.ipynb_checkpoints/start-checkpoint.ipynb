{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import os\n",
    "os.chdir('../')\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tkinter import TclError\n",
    "\n",
    "# use this backend to display in separate Tk window\n",
    "%matplotlib tk\n",
    "\n",
    "# constants\n",
    "CHUNK = 1024 * 2             # samples per frame\n",
    "FORMAT = pyaudio.paInt16     # audio format (bytes per sample?)\n",
    "CHANNELS = 1                 # single channel for microphone\n",
    "RATE = 44100                 # samples per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream started\n",
      "stream stopped\n",
      "average frame rate = 21 FPS\n"
     ]
    }
   ],
   "source": [
    "# create matplotlib figure and axes\n",
    "fig, ax = plt.subplots(1, figsize=(15, 7))\n",
    "\n",
    "# pyaudio class instance\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# stream object to get data from microphone\n",
    "stream = p.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    output=True,\n",
    "    frames_per_buffer=CHUNK\n",
    ")\n",
    "\n",
    "# variable for plotting\n",
    "x = np.arange(0, 2 * CHUNK, 2)\n",
    "\n",
    "# create a line object with random data\n",
    "line, = ax.plot(x, np.random.rand(CHUNK), '-', lw=2)\n",
    "\n",
    "# basic formatting for the axes\n",
    "ax.set_title('AUDIO WAVEFORM')\n",
    "ax.set_xlabel('samples')\n",
    "ax.set_ylabel('volume')\n",
    "ax.set_ylim(0, 255)\n",
    "ax.set_xlim(0, 2 * CHUNK)\n",
    "plt.setp(ax, xticks=[0, CHUNK, 2 * CHUNK], yticks=[0, 128, 255])\n",
    "\n",
    "# show the plot\n",
    "plt.show(block=False)\n",
    "\n",
    "print('stream started')\n",
    "\n",
    "# for measuring frame rate\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # binary data\n",
    "    data = stream.read(CHUNK)  \n",
    "    \n",
    "    # convert data to integers, make np array, then offset it by 127\n",
    "    data_int = struct.unpack(str(2 * CHUNK) + 'B', data)\n",
    "    \n",
    "    # create np array and offset by 128\n",
    "    data_np = np.array(data_int, dtype='b')[::2] + 128\n",
    "    \n",
    "    line.set_ydata(data_np)\n",
    "    \n",
    "    # update figure canvas\n",
    "    try:\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        frame_count += 1\n",
    "        \n",
    "    except TclError:\n",
    "        \n",
    "        # calculate average frame rate\n",
    "        frame_rate = frame_count / (time.time() - start_time)\n",
    "        \n",
    "        print('stream stopped')\n",
    "        print('average frame rate = {:.0f} FPS'.format(frame_rate))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training Dataset Preparation\n",
    "\n",
    "Input to model : 8 sec of audio clip with My sound Dubbed in between . \n",
    "\n",
    "Inference : Model listens for 8 second for a while. Then, processes it . waits for a while. Then, Processes ..\n",
    "\n",
    "More like, multithread, such that, by time it processes for prediction generation, it's preparing next wave .\n",
    "\n",
    "The computaiton control is done by time.sleep .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython.display.Audio(\"data/external/sample_internet.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "song = AudioSegment.from_wav('/home/fm-pc-lt-151/snap/audacity/748/training_data.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydub.audio_segment.AudioSegment"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load training examles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/fm-pc-lt-151/podcast_research/Podcast-Audio-Processing/data/external/Deep-Learning-Coursera-master/Sequence Models/Week3/Trigger word detection/raw_data/activates/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Clip Vol :  -28.489737167279724\n",
      "Traning Clip Vol :  -25.508003256941404\n",
      "Traning Clip Vol :  -29.95164431992424\n",
      "Traning Clip Vol :  -33.16234877056899\n",
      "Traning Clip Vol :  -25.37410450471753\n",
      "Traning Clip Vol :  -29.664684633234724\n",
      "Traning Clip Vol :  -24.5798693042547\n",
      "Traning Clip Vol :  -29.201432071694352\n",
      "Traning Clip Vol :  -30.17112454023635\n",
      "Traning Clip Vol :  -29.301403573965196\n",
      "Traning Clip Vol :  -27.812395710917173\n",
      "Traning Clip Vol :  -24.28405692647013\n",
      "Traning Clip Vol :  -34.77497501942614\n",
      "Traning Clip Vol :  -34.363647882580025\n",
      "Traning Clip Vol :  -35.28341661952751\n"
     ]
    }
   ],
   "source": [
    "root_ = '/home/fm-pc-lt-151/snap/audacity/748/'\n",
    "clips = []\n",
    "for i in os.listdir(root_):\n",
    "    if i[0].isdigit():\n",
    "        clip_path = os.path.join( root_ , i ) \n",
    "        clip = AudioSegment.from_wav(clip_path)\n",
    "        print('Traning Clip Vol : ' , clip.dBFS )\n",
    "        clips.append( clip )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_song_length = song.duration_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.playback import play\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "play( song[23.9999999 * n : 29 * n] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_activate( clip_list ):\n",
    "    return random.choice(clip_list)\n",
    "\n",
    "## experimental \n",
    "def adjust_level(sound, deviation=None , default = True):\n",
    "    if default or deviation is None:\n",
    "        return sound\n",
    "    else:\n",
    "        difference = np.random.normal(clip.dBFS , deviation )\n",
    "        print(f'Original : {clip.dBFS}')\n",
    "        print(f'Deviated by : {difference}')\n",
    "        return sound.apply_gain(difference)\n",
    "\n",
    "def sample_from_background( background , total_duration_sec , clip_size = 8 ,  multiplier = 1000 ):\n",
    "        \n",
    "    total_duration_ = total_duration_sec - clip_size\n",
    "    selection_start = np.random.uniform(  0 , total_duration_  ) \n",
    "    selection_end = (selection_start + clip_size) * multiplier\n",
    "        \n",
    "    background_clip = background[ selection_start * multiplier : selection_end  ] \n",
    "    return background_clip\n",
    "\n",
    "def overlay_clip_to_bg( background , clips ):\n",
    "    '''\n",
    "        Dub Clip to background\n",
    "    '''\n",
    "    overlay_point = np.random.uniform( 7 , background.duration_seconds ) * 1000\n",
    "    return background.overlay( random.choice( clips ) , position =  overlay_point ) , overlay_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 s, sys: 0 ns, total: 1.6 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## check the goddamn speed\n",
    "for i in range(30000):\n",
    "    _ = sample_from_background(song, total_song_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_example( background , voice_clips ):\n",
    "    \n",
    "    total_song_length = background.duration_seconds \n",
    "    background = sample_from_background( background , 1 )\n",
    "\n",
    "    ## positive labels here\n",
    "    if np.random.uniform(0,1) > 0.5:\n",
    "        clip_to_dub = sample_from_activate(clips)\n",
    "        overlayed_clip, time_stamp = overlay_clip_to_bg( background, clip_to_dub )  \n",
    "        return overlayed_clip , time_stamp\n",
    "    \n",
    "    else:\n",
    "        return background , None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overlay , overlay_point = generate_single_example( song , clips )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    <audio controls>\n",
       "                        <source src=\"data:audio/mpeg;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU3LjgzLjEwMAAAAAAAAAAAAAAA//tAwAAAAAAAAAAAAAAAAAAAAAAASW5mbwAAAA8AAAAAAAAAtgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAATGF2YzU3LjEwAAAAAAAAAAAAAAAAAAAAAAAAAAAAALYAAFu3AAAAAAA=\" type=\"audio/mpeg\"/>\n",
       "                        Your browser does not support the audio element.\n",
       "                    </audio>\n",
       "                  "
      ],
      "text/plain": [
       "<pydub.audio_segment.AudioSegment at 0x7f91f33b7090>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydub.audio_segment.AudioSegment"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio procesing",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
